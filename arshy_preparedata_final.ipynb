{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLdQF_bMxiAM"
      },
      "source": [
        "In this notebook, we will see how we can preprocess the data that was downloaded from Open Neuro. \n",
        "\n",
        "The preproces we used \n",
        "- `filter` to filter the signals between desired Hz\n",
        "- `resample` to resample the eeg signal from acqusition frequency to a desired frequency\n",
        "- `ICA` to remove `ecg, eog` related artifacts \n",
        "- `fized length epochs` to break the continuous signal to number of samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CdVHs4ugLLV"
      },
      "source": [
        "The following approaches were used to preproces that data\n",
        "\n",
        "Approach 1\n",
        "\n",
        "- filter between 1 and 40\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA\n",
        "- epoch 50s\n",
        "\n",
        "Approach 2\n",
        "\n",
        "- filter between 1 and 40\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA\n",
        "- epoch 50s and average\n",
        "\n",
        "Approach 3\n",
        "\n",
        "- filter between 1 and 20\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA on epochs\n",
        "- epoch 50s\n",
        "\n",
        "Approach 4\n",
        "\n",
        "- filter between 1 and 20\n",
        "- resample from 500hz to 100 hz\n",
        "- remove artifacts based on ICA on epochs\n",
        "- epoch 50s and average\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eZ9teOhqog_x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install fastcore mne mne-bids PyQt5 -Uqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RQqpi4AGYkYO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib\n",
        "from pathlib import Path\n",
        "import mne\n",
        "import mne_bids\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from fastcore.parallel import parallel\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZV4m9Rr3veqA"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import TransformerMixin,BaseEstimator\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aCDNfYWkrjd9"
      },
      "outputs": [],
      "source": [
        "def prepare_approach12(fn):\n",
        "    '''\n",
        "    approach1\n",
        "    filter - 1 and 40\n",
        "    resample - 100hz\n",
        "    ica - 20 components\n",
        "    epoch - 50s\n",
        "\n",
        "    approach2\n",
        "    same but average all epochs to one   \n",
        "    '''\n",
        "    path = f\"processeddata/individuals/afterica\"\n",
        "    sub = str(fn).split('/')[-1].split('_')[0]\n",
        "    session = str(fn).split('/')[-1].split('_')[1]\n",
        "    label = str(fn).split('/')[-1].split('_')[2]\n",
        "\n",
        "    read_raw_bids(bids_path=fn, verbose=False)\n",
        "    raw.load_data()\n",
        "    raw = raw.resample(100).filter(l_freq=1, h_freq=40)\n",
        "\n",
        "    ica = mne.preprocessing.ICA(n_components=20, \n",
        "                                random_state=0)\n",
        "    ica.fit(raw)\n",
        "\n",
        "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
        "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
        "    \n",
        "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
        "\n",
        "    raw_after = ica.apply(raw, \n",
        "                          exclude=ica.exclude)\n",
        "\n",
        "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
        "                                                duration=50,  \n",
        "                                                overlap=0)\n",
        "\n",
        "    fn1 = f\"{path}/{label}_{sub}_{session}_approach1.npy\"\n",
        "    fn2 = f\"{path}/{label}_{sub}_{session}_approach2.npy\"\n",
        "\n",
        "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
        "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sEZc7zM9hp0s"
      },
      "outputs": [],
      "source": [
        "def prepare_approach34(fn):\n",
        "    '''\n",
        "    approach1\n",
        "    filter - 1 and 20\n",
        "    resample - 100hz\n",
        "    ica - 20 components\n",
        "    epoch - 50s\n",
        "\n",
        "    approach2\n",
        "    same but average all epochs to one   \n",
        "    '''\n",
        "    path = f\"processeddata/individuals/afterica\"\n",
        "    sub = str(fn).split('/')[-1].split('_')[0]\n",
        "    session = str(fn).split('/')[-1].split('_')[1]\n",
        "    label = str(fn).split('/')[-1].split('_')[2]\n",
        "\n",
        "    raw = mne.io.read_raw_brainvision(fn, preload=True)\n",
        "    raw = raw.resample(100).filter(l_freq=1, h_freq=20)\n",
        "\n",
        "    ica = mne.preprocessing.ICA(n_components=20, \n",
        "                                random_state=0)\n",
        "    ica.fit(raw)\n",
        "    bad_idx_ecg, scores_ecg = ica.find_bads_ecg(raw, 'Fp1', threshold=2)\n",
        "    bad_idx_eog, scores_eog = ica.find_bads_eog(raw, 'Fp1', threshold=2)\n",
        "    ica.exclude = bad_idx_ecg + bad_idx_eog\n",
        "\n",
        "    raw_after = ica.apply(raw, \n",
        "                          exclude=ica.exclude)\n",
        "\n",
        "    epochs_after = mne.make_fixed_length_epochs(raw_after,  \n",
        "                                                duration=50,  \n",
        "                                                overlap=0)\n",
        "\n",
        "    fn1 = f\"{path}/{label}_{sub}_{session}_approach3.npy\"\n",
        "    fn2 = f\"{path}/{label}_{sub}_{session}_approach4.npy\"\n",
        "\n",
        "    np.save(fn1, epochs_after.get_data().astype(np.float16))\n",
        "    np.save(fn2, epochs_after.average().get_data().astype(np.float16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.datasets import sample\n",
        "import os.path as op\n",
        "\n",
        "dataset = 'ds003685'\n",
        "bids_root = op.join(op.dirname(sample.data_path()), dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ilub4vcmuj6x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ben/mne_data/ds003685\n"
          ]
        }
      ],
      "source": [
        "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
        "\n",
        "print(bids_root)\n",
        "datatype = 'eeg'\n",
        "suffix = 'eeg'\n",
        "bids_paths = [ \n",
        "    BIDSPath(task='eyes open', suffix=suffix, datatype=datatype, root=bids_root),\n",
        "    BIDSPath(task='eyes closed', suffix=suffix, datatype=datatype, root=bids_root)]\n",
        "\n",
        "# filenames = []\n",
        "# for path in Path('hackdataset').rglob('*.vhdr'):\n",
        "#     filenames.append(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 53926.77it/s]\n",
            "100%|██████████| 9/9 [00:00<00:00, 61181.10it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_vhdr_files(bids_path):\n",
        "    # get names of the .vhdr files\n",
        "    basenames = []\n",
        "    match = bids_path.match()\n",
        "    for i in tqdm(range(0, len(match))):\n",
        "        if match[i].basename[-4:] == 'vhdr':\n",
        "            basenames.append(match[i])\n",
        "    return basenames\n",
        "\n",
        "vhdr_files = []\n",
        "for bids_path in bids_paths:\n",
        "    vhdr_files.append(get_vhdr_files(bids_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:00<00:00,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [00:00<00:00,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:00<00:00,  5.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [00:00<00:00,  5.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading 0 ... 149999  =      0.000 ...   299.998 secs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  5.73it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_raw_files(basenames):\n",
        "    raws = []\n",
        "    for basename in tqdm(basenames):\n",
        "        raw = read_raw_bids(bids_path=basename, verbose=False)\n",
        "        raw.load_data()\n",
        "        raws.append(raw)\n",
        "    \n",
        "#print(vhdr_files)\n",
        "open_eyes = get_raw_files(vhdr_files[0])\n",
        "closed_eyes = get_raw_files(vhdr_files[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DMYn6EZAvvIl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# for fn in open_eyes:\n",
        "#   print(x)\n",
        "parallel(prepare_approach12, \n",
        "         open_eyes, \n",
        "         n_workers=8, \n",
        "         progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4v5rjatSv7np"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "parallel(prepare_approach34, \n",
        "         open_eyes, \n",
        "         n_workers=8, \n",
        "         progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I4kFn-rONB_"
      },
      "source": [
        "# Prepare df and Standard Scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cCpw_3huvhf5"
      },
      "outputs": [],
      "source": [
        "class SScaler3D(BaseEstimator,TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        self.scaler.fit(X.reshape(X.shape[0], -1))\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        return self.scaler.transform(X.reshape(X.shape[0], -1)).reshape(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EpFQzzmnOMQa"
      },
      "outputs": [],
      "source": [
        "def prepare_df_ss(approach):\n",
        "    fns = glob(f'processeddata/individuals/afterica/*{approach}.npy')\n",
        "\n",
        "    fns_2 = []\n",
        "    label = []\n",
        "    subject = []\n",
        "    session = []\n",
        "    for fn in fns:\n",
        "        fns_2.append(fn)\n",
        "        label.append(str(fn).split('/')[-1].split('_')[0])\n",
        "        subject.append(str(fn).split('/')[-1].split('_')[1])\n",
        "        session.append(str(fn).split('/')[-1].split('_')[2])\n",
        "\n",
        "    df = pd.DataFrame([fns_2, label, subject, session]).T\n",
        "    df.columns = ['fns', 'label', 'subject', 'session']\n",
        "\n",
        "    val_subs = ['sub-51', 'sub-52', 'sub-53', 'sub-54', 'sub-55', 'sub-56', 'sub-57', 'sub-58', 'sub-59', 'sub-60']\n",
        "    df['is_valid'] = False\n",
        "    df.loc[df[df['subject'].isin(val_subs)].index, 'is_valid'] = True\n",
        "\n",
        "    df.to_csv(f'{approach}infos.csv', index=False)\n",
        "\n",
        "    if approach in ['approach1', 'approach3']:\n",
        "        features = []\n",
        "        for fn in df[df['is_valid']==False].fns.values:\n",
        "            if np.load(fn).shape[0] == 6:\n",
        "                features.append(np.load(fn))\n",
        "        \n",
        "        features = np.stack(features, 0)\n",
        "        ss = SScaler3D()\n",
        "        ss.fit(features)\n",
        "\n",
        "        pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))\n",
        "\n",
        "    else:\n",
        "      features = []\n",
        "      for fn in df[df['is_valid']==False].fns.values:\n",
        "          features.append(np.load(fn))\n",
        "      \n",
        "      features = np.stack(features, 0)\n",
        "      ss = StandardScaler()\n",
        "      ss.fit(features)\n",
        "\n",
        "      pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ru6QsKRazw2p"
      },
      "outputs": [],
      "source": [
        "fns = glob(f'processeddata/individuals/afterica/*approach1.npy')\n",
        "\n",
        "fns_2 = []\n",
        "label = []\n",
        "subject = []\n",
        "session = []\n",
        "for fn in fns:\n",
        "    fns_2.append(fn)\n",
        "    label.append(str(fn).split('/')[-1].split('_')[0])\n",
        "    subject.append(str(fn).split('/')[-1].split('_')[1])\n",
        "    session.append(str(fn).split('/')[-1].split('_')[2])\n",
        "\n",
        "df = pd.DataFrame([fns_2, label, subject, session]).T\n",
        "df.columns = ['fns', 'label', 'subject', 'session']\n",
        "\n",
        "val_subs = ['sub-51', 'sub-52', 'sub-53', 'sub-54', 'sub-55', 'sub-56', 'sub-57', 'sub-58', 'sub-59', 'sub-60']\n",
        "df['is_valid'] = False\n",
        "df.loc[df[df['subject'].isin(val_subs)].index, 'is_valid'] = True\n",
        "\n",
        "df.to_csv(f'approach1infos.csv', index=False)\n",
        "\n",
        "\n",
        "# features = np.stack(features, 0)\n",
        "# if approach in ['approach1', 'approach3']:\n",
        "#     ss = SScaler3D()\n",
        "#     ss.fit(features)\n",
        "\n",
        "#     pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))\n",
        "\n",
        "# else:\n",
        "#   ss = StandardScaler()\n",
        "#   ss.fit(features)\n",
        "\n",
        "#   pickle.dump(ss, open(f'ss_{approach}.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2KQqLYeO0Ik8"
      },
      "outputs": [],
      "source": [
        "features = []\n",
        "for fn in df[df['is_valid']==False].fns.values:\n",
        "    if np.load(fn).shape[0] == 6:\n",
        "        features.append(np.load(fn))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n8h8Ql_i0Mc7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "need at least one array to stack",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/7b/xbwtz8m5467_89kxyf1qm6tw0000gp/T/ipykernel_12409/2844918415.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
          ]
        }
      ],
      "source": [
        "features = np.stack(features, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "XCcdYsFAxAFM",
        "outputId": "1a8f7012-7082-4a2f-968b-65feb503b4eb"
      },
      "outputs": [],
      "source": [
        "prepare_df_ss('approach1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "y0OSyDCNxHoL",
        "outputId": "ff8399a3-375a-4ca5-f54f-bc9c8789072f"
      },
      "outputs": [],
      "source": [
        "prepare_df_ss('approach2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v068lSxFxKnB"
      },
      "outputs": [],
      "source": [
        "prepare_df_ss('approach3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1dLkdE5xLmI"
      },
      "outputs": [],
      "source": [
        "prepare_df_ss('approach4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KGQh6dnQxRcj",
        "outputId": "2f5d0623-481c-4dae-8db2-bca390420ae8"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFTgwp1s2PKO",
        "outputId": "0852b016-746b-49c0-ed8a-0868832f9221"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rw946Ub2Wdp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "arshy_preparedata_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
